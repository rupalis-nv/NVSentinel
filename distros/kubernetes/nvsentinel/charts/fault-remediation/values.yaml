
# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

replicaCount: 1

logLevel: info

# Client certificate mount path for database connections
clientCertMountPath: /etc/ssl/client-certs

image:
  repository: ghcr.io/nvidia/nvsentinel/fault-remediation
  pullPolicy: IfNotPresent
  tag: ""

resources: 
  limits:
    cpu: "200m"
    memory: "300Mi"
  requests:
    cpu: "200m"
    memory: "300Mi"

# Scheduling configuration
nodeSelector: {}
affinity: {}

podAnnotations: {}

# Special tolerations for fault remediation - allow running on nodes with any taints for log collection
tolerations:
  - operator: "Exists"

# Multi-template remediation configuration
# Allows different remediation actions to use different CRDs and operators
maintenance:
  # Per-action remediation definitions
  # Key is the RecommendedAction string (e.g., "COMPONENT_RESET", "RESTART_BM") 
  actions:
    COMPONENT_RESET:
      apiGroup: "janitor.dgxc.nvidia.com"
      version: "v1alpha1"
      kind: "RebootNode"
      scope: "Cluster"
      completeConditionType: "NodeReady"
      templateFileName: "nvidia-reboot.yaml"
      equivalenceGroup: "restart"
    # Additional action examples:
    # RESTART_BM:
    #   apiGroup: "remediation.example.com"
    #   version: "v1alpha1"
    #   kind: "RestartNode"
    #   scope: "Namespaced"
    #   namespace: "remediation" 
    #   completeConditionType: "NodeReady"
    #   templateFileName: "namespaced-restart.yaml"
    #   equivalenceGroup: "restart"  # Same group as COMPONENT_RESET
    #
    # NOTE: Resource names for RBAC are generated by appending 's' to lowercase kind.
    # This works for regular nouns but may fail for irregular plurals:
    #   RebootNode → rebootnodes (✓ correct)
    #   Policy → policys (✗ should be policies)
    # Use CRD kinds that follow regular pluralization rules.

  # Template content for each remediation action
  # Key matches the templateFileName name from actions above
  templates:
    "nvidia-reboot.yaml": |
      apiVersion: janitor.dgxc.nvidia.com/v1alpha1
      kind: RebootNode
      metadata:
        name: maintenance-{{ .NodeName }}-{{ .HealthEventID }}
        labels:
          app.kubernetes.io/managed-by: nvsentinel
      spec:
        nodeName: {{ .NodeName }}
        force: false
    # Additional template examples:
    # "namespaced-restart.yaml": |
    #   apiVersion: remediation.example.com/v1alpha1 
    #   kind: RestartNode
    #   metadata:
    #     name: maintenance-{{ .NodeName }}-{{ .HealthEventID }}
    #     namespace: remediation
    #     labels:
    #       app.kubernetes.io/managed-by: nvsentinel
    #   spec:
    #     nodeName: {{ .NodeName }}
    #     ttlSecondsAfterFinished: 60

# Retry configuration for maintenance resource updates
# Used when updating annotations on nodes after creating maintenance resources
updateRetry:
  # Maximum number of retry attempts if the update fails (due to conflicts, network issues, etc.)
  maxRetries: 5
  # Delay in seconds between retry attempts (uses exponential backoff)
  retryDelaySeconds: 10

# Log collector configuration
# When enabled, creates a Kubernetes Job to collect diagnostic logs from failing nodes
logCollector:
  # Enable log collection jobs on node failures
  enabled: false
  image:
    repository: ghcr.io/nvidia/nvsentinel/log-collector
    # tag: latest  # Optional: Override global.image.tag for log-collector
    pullPolicy: IfNotPresent
  # HTTP endpoint where collected logs will be uploaded
  uploadURL: "http://nvsentinel-incluster-file-server.nvsentinel.svc.cluster.local/upload"
  # Comma-separated list of namespaces where GPU operator components run (for collecting GPU operator logs)
  gpuOperatorNamespaces: "gpu-operator"
  # Enable GPU Operator must-gather collection (disabled by default)
  # WARNING: must-gather collects logs from ALL nodes in the cluster, which can be very
  # time-consuming for large clusters (e.g., GB200 clusters with 100+ nodes).
  # If enabling this, you MUST increase the timeout accordingly.
  # Recommended timeout: ~2-3 minutes per node (e.g., 100 nodes = 200-300 minutes)
  enableGpuOperatorMustGather: false
  # Enable GCP-specific SOS report collection
  enableGcpSosCollection: false
  # Enable AWS-specific SOS report collection
  enableAwsSosCollection: false
  # Timeout for waiting for log collection job to complete
  # Default: 10m (sufficient for nvidia-bug-report only)
  # If enableGpuOperatorMustGather is true, increase to: ~2-3 min per node in cluster
  timeout: "10m"
  # Additional environment variables to pass to the log-collector container
  # Example:
  # env:
  #   MOCK_MODE: "true"
  #   DEBUG: "1"
  env: {}
